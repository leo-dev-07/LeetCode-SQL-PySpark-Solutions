{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b89f5d7b832c834",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# [570. Managers with at Least 5 Direct Reports](https://leetcode.com/problems/managers-with-at-least-5-direct-reports/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f0434a396216b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Table: Employee\n",
    "\n",
    "<pre>+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| department  | varchar |\n",
    "| managerId   | int     |\n",
    "+-------------+---------+</pre>\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the name of an employee, their department, and the id of their manager.\n",
    "If managerId is null, then the employee does not have a manager.\n",
    "No employee will be the manager of themself.\n",
    " \n",
    "\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "<pre>+-----+-------+------------+-----------+\n",
    "| id  | name  | department | managerId |\n",
    "+-----+-------+------------+-----------+\n",
    "| 101 | John  | A          | None      |\n",
    "| 102 | Dan   | A          | 101       |\n",
    "| 103 | James | A          | 101       |\n",
    "| 104 | Amy   | A          | 101       |\n",
    "| 105 | Anne  | A          | 101       |\n",
    "| 106 | Ron   | B          | 101       |\n",
    "+-----+-------+------------+-----------+</pre>\n",
    "Output: \n",
    "<pre>+------+\n",
    "| name |\n",
    "+------+\n",
    "| John |\n",
    "+------+</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cd1ba66f6a6cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:00:55.617190700Z",
     "start_time": "2023-11-05T19:00:54.882279800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas schema\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e00eb4aa7c62213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:00:55.751997100Z",
     "start_time": "2023-11-05T19:00:55.616132800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to pyspark schema\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7cde997baaa96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:01:12.008869400Z",
     "start_time": "2023-11-05T19:00:55.732960Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "|id |name |department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101|John |A         |NULL     |\n",
      "|102|Dan  |A         |101      |\n",
      "|103|James|A         |101      |\n",
      "|104|Amy  |A         |101      |\n",
      "|105|Anne |A         |101      |\n",
      "|106|Ron  |B         |101      |\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 63466)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\site-packages\\pyspark\\accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\site-packages\\pyspark\\accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\site-packages\\pyspark\\serializers.py\", line 595, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"c:\\Users\\lpaschoal\\miniconda3\\envs\\data-ai\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "employee_df = spark.createDataFrame(data,schema=schema)\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c647b45dc8ee9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:01:15.111346200Z",
     "start_time": "2023-11-05T19:01:12.006298600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# in Spark Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe69d8b8cc3845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:01:16.885108100Z",
     "start_time": "2023-11-05T19:01:15.100758800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n"
     ]
    }
   ],
   "source": [
    "# in Spark SQL\n",
    "employee_df.createOrReplaceTempView('employee')\n",
    "spark.sql('''\n",
    "select name\n",
    "from employee\n",
    "where id in (select managerId from employee group by managerId having count(managerId)>=5)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d0d3e7837236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:01:17.652441100Z",
     "start_time": "2023-11-05T19:01:16.867080500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c60351e9251ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T19:01:17.699399100Z",
     "start_time": "2023-11-05T19:01:17.645279500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
